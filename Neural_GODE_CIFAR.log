import os
import argparse
import logging
import time
import numpy as np
import torch
import torch.nn as nn
from torch.utils.data import DataLoader
import torchvision.datasets as datasets
import torchvision.transforms as transforms
import math
from torch.nn import functional as F

def conv3x3(in_planes, out_planes, stride=1):  # filter size 3
    """3x3 convolution with padding"""
    return nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride, padding=1, bias=False)


def conv1x1(in_planes, out_planes, stride=1):  # filter size 1
    """1x1 convolution"""
    return nn.Conv2d(in_planes, out_planes, kernel_size=1, stride=stride, bias=False)


def norm(dim): # apply a group normalization over a mini-batch of input; 
    #The input channels are separated into num_groups groups, each containing num_channels/num_groups channels.
    return nn.GroupNorm(min(32, dim), dim)  # number of groups: min(32, dim); number of channels: dim
    
def basisF(u, k, i, t):     # the basis function based on recursive formula
    if k == 0:
        return 1.0 if t[i] <= u < t[i+1] else 0.0
    if t[i+k] == t[i]:
        s1 = 0.0
    else:
        s1 = (u - t[i])/(t[i+k] - t[i]) * basisF(u, k-1, i, t)
    if t[i+k+1] == t[i+1]:
        s2 = 0.0
    else:
        s2 = (t[i+k+1] - u)/(t[i+k+1] - t[i+1]) * basisF(u, k-1, i+1, t)
    return s1 + s2
    
class ResBlock(nn.Module):
    expansion = 1

    def __init__(self, inplanes, planes, stride=1, downsample=None):
        super(ResBlock, self).__init__()
        self.norm1 = norm(inplanes)           
        self.relu = nn.ReLU(inplace=True)     
        self.downsample = downsample
        self.conv1 = conv3x3(inplanes, planes, stride)
        self.norm2 = norm(planes)
        self.conv2 = conv3x3(planes, planes)

    def forward(self, x):
        shortcut = x

        out = self.relu(self.norm1(x))

        if self.downsample is not None:
            shortcut = self.downsample(out)

        out = self.conv1(out) 
        out = self.norm2(out) #trick of batch normalization: adopt the BN right after convolution before activation
        out = self.relu(out)
        out = self.conv2(out)

        return out + shortcut

class ConcatConv2d(nn.Module):

    def __init__(self, dim_in, dim_out, ksize=3, stride=1, padding=0, dilation=1, groups=1, bias=True, transpose=False):
        super(ConcatConv2d, self).__init__()
        module = nn.ConvTranspose2d if transpose else nn.Conv2d
        self._layer = module(
            dim_in + 1, dim_out, kernel_size=ksize, stride=stride, padding=padding, dilation=dilation, groups=groups,
            bias=bias
        )          # add another dimension for time

    def forward(self, t, x):
        tt = torch.ones_like(x[:, :1, :, :]) * t #fill the tensor with all 1, dimension as similar as x[:,:1,:,:]
        ttx = torch.cat([tt, x], 1)              #concatenate time with x, at dimension 1. 
        return self._layer(ttx)

 # define a layer of linear combination with spline function as coefficients and bias    
class SplineLinear(nn.Module):
    
    def __init__(self, dim, spline_n=3,step_size=0.1):
        super(SplineLinear,self).__init__()                 # inherit the basic module
        
        # parameters
        self.dim = dim
        self.n = spline_n
        self.step_size = step_size
           
        # trainable coefficients of the spline function
        c1 = torch.Tensor(self.dim,self.dim, 3, 3, self.n)
        self.c1 = torch.nn.Parameter(c1)
        nn.init.kaiming_uniform_(self.c1, a= math.sqrt(3))

    
    def BS(self, t, c):     # calculate basis function
        t_index = int(torch.round(t/0.001).item())
        #print(t)
        Tele_spline = torch.Tensor(ele_spline[t_index]).to(device = device)
        BS_fvalue = torch.mul(c,Tele_spline)
        BS_fvalue = torch.sum(BS_fvalue,4)
        return BS_fvalue
    
    def spline_conv(self,t,x):    
        weights = self.BS(t, self.c1) # b-spline function as the cnn weights
        #conv_fun = nn.Conv2d(self.dim, self.dim, 3, stride=1, padding=1, bias=False)
        #with torch.no_grad():
        #    conv_fun.weight = nn.Parameter(weights)   
        return F.conv2d(x,weights,stride=1,padding=1,bias=None)
    
    def forward(self,t,x):
        out = self.spline_conv(t,x)
        return out
            

class ODEfunc(nn.Module):
    def __init__(self,dim,spline_n,step_size):
        super(ODEfunc, self).__init__()
        self.norm1 = norm(dim)
        self.relu1 = nn.ReLU(inplace=True)
        self.spline_linear1 = SplineLinear(dim,spline_n,step_size)
        self.norm2 = norm(dim)
        self.relu2 = nn.ReLU(inplace=True)
        self.spline_linear2 = SplineLinear(dim,spline_n,step_size)
        
        self.nfe = 0                          # number of function evaluation
        
    def forward(self,t,x):
        #print(x.shape)
        self.nfe += 1
        out = self.norm1(x)
        out = self.relu1(out)
        out = self.spline_linear1(t,out)
        out = self.norm2(out)
        out = self.relu2(out)
        out = self.spline_linear2(t,out)
  
        return out
        

class ODEBlock(nn.Module):

    def __init__(self, odefunc):
        super(ODEBlock, self).__init__()
        self.odefunc = odefunc
        self.integration_time = torch.tensor([0, args.int_T]).float()  # integrate from 0 to int_T

    def forward(self, x):
        self.integration_time = self.integration_time.type_as(x)
        #print(self.integration_time)
        #out = odeint(self.odefunc, x, self.integration_time, rtol=args.tol, atol=args.tol, method='adaptive_heun') # the ODE system
        out = odeint(self.odefunc, x, self.integration_time, rtol=args.tol, atol=args.tol, method = 'euler', options=dict(step_size=args.ode_step_size)) # the ODE system
        return out[1]

    @property
    def nfe(self):
        return self.odefunc.nfe

    @nfe.setter
    def nfe(self, value):
        self.odefunc.nfe = value


class Flatten(nn.Module):

    def __init__(self):
        super(Flatten, self).__init__()

    def forward(self, x):
        shape = torch.prod(torch.tensor(x.shape[1:])).item()
        #print(x.shape)
        #print(x.view(-1, shape).shape)
        return x.view(-1, shape)

class RunningAverageMeter(object):
    """Computes and stores the average and current value"""

    def __init__(self, momentum=0.99):
        self.momentum = momentum
        self.reset()

    def reset(self):
        self.val = None
        self.avg = 0

    def update(self, val):
        if self.val is None:
            self.avg = val
        else:
            self.avg = self.avg * self.momentum + val * (1 - self.momentum)
        self.val = val
        #print('value:'+str(self.val))
        #print(self.avg)
        


def get_mnist_loaders(data_aug=False, batch_size=128, test_batch_size=1000, perc=1.0):
    normalize = transforms.Normalize(mean=[0.491, 0.482, 0.447], std=[0.247, 0.243, 0.262])
    if data_aug:
        transform_train = transforms.Compose([       # chain the transformation together using compose
            transforms.RandomCrop(32, padding=4),    # crop the image at random location with padding 
            transforms.ToTensor(),
            normalize,
        ])
    else:
        transform_train = transforms.Compose([
            transforms.ToTensor(),
            normalize,
        ])

    transform_test = transforms.Compose([
        transforms.ToTensor(),
        normalize,
    ])

    train_loader = DataLoader(
        datasets.CIFAR10(root='.data/cifar10', train=True, download=True, transform=transform_train), batch_size=batch_size,
        shuffle=True, num_workers=2, drop_last=True
    )

    train_eval_loader = DataLoader(
        datasets.CIFAR10(root='.data/cifar10', train=True, download=True, transform=transform_test),
        batch_size=test_batch_size, shuffle=False, num_workers=2, drop_last=True
    )

    test_loader = DataLoader(
        datasets.CIFAR10(root='.data/cifar10', train=False, download=True, transform=transform_test),
        batch_size=test_batch_size, shuffle=False, num_workers=2, drop_last=True
    )

    return train_loader, test_loader, train_eval_loader


def inf_generator(iterable):  # a object is iterable if we can get an iterator from it
    """Allows training with DataLoaders in a single infinite loop:
        for i, (x, y) in enumerate(inf_generator(train_loader)):
    """
    iterator = iterable.__iter__()  # define an interator
    while True:
        try:
            yield iterator.__next__() # get the element of next iterator
        except StopIteration:
            iterator = iterable.__iter__()


def learning_rate_with_decay(batch_size, batch_denom, batches_per_epoch, boundary_epochs, decay_rates):
    initial_learning_rate = args.lr * batch_size / batch_denom
    #initial_learning_rate = lr * batch_size / batch_denom

    boundaries = [int(batches_per_epoch * epoch) for epoch in boundary_epochs]
    vals = [initial_learning_rate * decay for decay in decay_rates]

    def learning_rate_fn(itr):
        lt = [itr < b for b in boundaries] + [True]
        i = np.argmax(lt)
        return vals[i]

    return learning_rate_fn

def one_hot(x, K):
    return np.array(x[:, None] == np.arange(K)[None, :], dtype=int)


def accuracy(model, dataset_loader):
    total_correct = 0
    for x, y in dataset_loader:
        x = x.to(device)
        y = one_hot(np.array(y.numpy()), 10)

        target_class = np.argmax(y, axis=1)
        predicted_class = np.argmax(model(x).cpu().detach().numpy(), axis=1)
        total_correct += np.sum(predicted_class == target_class)
    return total_correct / len(dataset_loader.dataset)


def count_parameters(model):
    return sum(p.numel() for p in model.parameters() if p.requires_grad)


def makedirs(dirname):
    if not os.path.exists(dirname):
        os.makedirs(dirname)


def get_logger(logpath, filepath, package_files=[], displaying=True, saving=True, debug=False):
    logger = logging.getLogger()  # 1. setup a logger
    if debug:
        level = logging.DEBUG
    else:
        level = logging.INFO
    logger.setLevel(level)     # 2. define the cut-point of level, logger levels have default priority 
    if saving:
        info_file_handler = logging.FileHandler(logpath, mode="a")
        info_file_handler.setLevel(level)
        logger.addHandler(info_file_handler)
    if displaying:
        console_handler = logging.StreamHandler()
        console_handler.setLevel(level)
        logger.addHandler(console_handler)
    logger.info(filepath)
    with open(filepath, "r") as f:
        logger.info(f.read())

    for f in package_files:
        logger.info(f)
        with open(f, "r") as package_f:
            logger.info(package_f.read())

    return logger

parser = argparse.ArgumentParser()
parser.add_argument('--network', type=str, choices=['resnet', 'odenet'], default='odenet')
parser.add_argument('--tol', type=float, default=1e-3)
parser.add_argument('--adjoint', type=eval, default=False, choices=[True, False])
parser.add_argument('--downsampling-method', type=str, default='conv', choices=['conv', 'res'])
parser.add_argument('--nepochs', type=int, default=10)
parser.add_argument('--data_aug', type=eval, default=True, choices=[True, False])
parser.add_argument('--lr', type=float, default=0.1)
parser.add_argument('--batch_size', type=int, default=128)
parser.add_argument('--test_batch_size', type=int, default=1000)
parser.add_argument('--save', type=str, default='./NeuralODE_CIFAR10_experiment/NeuralODE_CIFAR10_Bspline_convTest')
parser.add_argument('--debug', action='store_true')
parser.add_argument('--gpu', type=int, default=0)
parser.add_argument('--spline_n', type=int, default=3)  # number of coefficients 
parser.add_argument('--spline_k', type=int, default=2)  # degree of B-spline
parser.add_argument('--int_T', type=int, default = 1)
parser.add_argument('--ode_step_size', type=float, default=0.01)
parser.add_argument('--num_equations', type=int, default = 10)
args = parser.parse_args()

tt = np.append(np.arange(0,args.int_T, args.int_T/(args.spline_k + args.spline_n)),[args.int_T]) # knots vector
ele_spline = [[basisF(t, args.spline_k, i, tt) for i in range(args.spline_n)] for t in np.arange(0,4,0.001)]

if args.adjoint:
        from torchdiffeq import odeint_adjoint as odeint
else:
        from torchdiffeq import odeint
if __name__ == '__main__':
    makedirs(args.save)
    logger = get_logger(logpath=os.path.join(args.save, 'logs'), filepath=os.path.abspath(__file__))
    logger.info(args)
    device = torch.device('cuda:' + str(args.gpu) if torch.cuda.is_available() else 'cpu')
    is_odenet = args.network == 'odenet'

    if args.downsampling_method == 'conv':
        downsampling_layers = [
            nn.Conv2d(3, 64, 3, 1),   # in_channels = 1, out_channels = 64, filter_size = 3, stride = 1, outdim = 126
            # number_parameters = 3*3*64+64 = 640
            norm(64),                # group normalization
            # number_parameters = 64*2 = 128
            nn.ReLU(inplace=True),
            # number_parameters = 0
            nn.Conv2d(64, 64, 4, 2, 1), #in_channels = 64, out_channels = 64, filter_size =4, stride = 2, padding = 1
            # number_parameters = 4*4*64 (input_channels)*64+64 = 65600, output_dim = 63
            norm(64),
            # number_parameters = 64*2
            nn.ReLU(inplace=True),
            # number_parameters = 0
            nn.Conv2d(64, 64, 4, 2, 1), # output: 64 filter map with dimension 
            # number_parameters = 65600
            #norm(64),
            #nn.ReLU(inplace=True),
        ]   # total number of parameters = 132096
    elif args.downsampling_method == 'res':
        downsampling_layers = [
            nn.Conv2d(1, 64, 3, 1),
            ResBlock(64, 64, stride=2, downsample=conv1x1(64, 64, 2)),
            ResBlock(64, 64, stride=2, downsample=conv1x1(64, 64, 2)),
        ]

    fc_layers_1 = [norm(64), nn.ReLU(inplace=True), nn.AdaptiveAvgPool2d((1, 1)),Flatten()]
    fc_layers_2 = [nn.Linear(args.num_equations, 10)]
    feature_layers = [ODEBlock(ODEfunc(args.num_equations,args.spline_n,args.ode_step_size))]
    model = nn.Sequential(*downsampling_layers, *feature_layers,*fc_layers_1,*fc_layers_2).to(device)

    logger.info(model)
    logger.info('Number of parameters: {}'.format(count_parameters(model)))

    criterion = nn.CrossEntropyLoss().to(device)  # combine softmax and loss 

    train_loader, test_loader, train_eval_loader = get_mnist_loaders(   # loading function
        args.data_aug, args.batch_size, args.test_batch_size
    )

    data_gen = inf_generator(train_loader) # infinite loop
    batches_per_epoch = len(train_loader)  # number of batches

    lr_fn = learning_rate_with_decay(
        args.batch_size, batch_denom=128, batches_per_epoch=batches_per_epoch, boundary_epochs=[60, 100, 140],
        decay_rates=[1, 0.1, 0.01, 0.001]
    )

    optimizer = torch.optim.SGD(model.parameters(), lr=args.lr, momentum=0.9)


    best_acc = 0
    batch_time_meter = RunningAverageMeter()
    f_nfe_meter = RunningAverageMeter()
    b_nfe_meter = RunningAverageMeter()
    end = time.time()
    
    for itr in range(args.nepochs * batches_per_epoch):       

        for param_group in optimizer.param_groups:
            param_group['lr'] = lr_fn(itr)

        optimizer.zero_grad()
        x, y = data_gen.__next__()
        x = x.to(device)
        y = y.to(device)
        logits = model(x)
        loss = criterion(logits, y)

        if is_odenet:
            nfe_forward = feature_layers[0].nfe
            feature_layers[0].nfe = 0

        loss.backward()
        optimizer.step()

        if is_odenet:
            nfe_backward = feature_layers[0].nfe
            feature_layers[0].nfe = 0

        batch_time_meter.update(time.time() - end)
        
        #print('value:'+str(batch_time_meter.val))
        #print(batch_time_meter.avg)
        if is_odenet:
            f_nfe_meter.update(nfe_forward)
            b_nfe_meter.update(nfe_backward)
        end = time.time()


        if itr % batches_per_epoch == 0:
            with torch.no_grad():
                train_acc = accuracy(model, train_eval_loader)
                val_acc = accuracy(model, test_loader)
                if val_acc > best_acc:
                    torch.save({'state_dict': model.state_dict(), 'args': args}, os.path.join(args.save, 'model.pth'))
                    best_acc = val_acc
                logger.info(
                    "Epoch {:04d} | Time {:.3f} ({:.3f}) | NFE-F {:.1f} | NFE-B {:.1f} | "
                    "Train Acc {:.4f} | Test Acc {:.4f}".format(
                        itr // batches_per_epoch, batch_time_meter.val, 
                        batch_time_meter.avg,f_nfe_meter.avg,
                        b_nfe_meter.avg, train_acc, val_acc)
                    )
                #log_time=[]

Namespace(adjoint=False, batch_size=128, data_aug=True, debug=False, downsampling_method='conv', gpu=0, int_T=1, lr=0.1, nepochs=160, network='odenet', num_equations=64, ode_step_size=0.05, save='./NeuralODE_CIFAR10_experiment/NeuralODE_CIFAR10_Bspline_convTest', spline_k=1, spline_n=8, test_batch_size=1000, tol=0.001)
Sequential(
  (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1))
  (1): GroupNorm(32, 64, eps=1e-05, affine=True)
  (2): ReLU(inplace=True)
  (3): Conv2d(64, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
  (4): GroupNorm(32, 64, eps=1e-05, affine=True)
  (5): ReLU(inplace=True)
  (6): Conv2d(64, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
  (7): ODEBlock(
    (odefunc): ODEfunc(
      (norm1): GroupNorm(32, 64, eps=1e-05, affine=True)
      (relu1): ReLU(inplace=True)
      (spline_linear1): SplineLinear()
      (norm2): GroupNorm(32, 64, eps=1e-05, affine=True)
      (relu2): ReLU(inplace=True)
      (spline_linear2): SplineLinear()
    )
  )
  (8): GroupNorm(32, 64, eps=1e-05, affine=True)
  (9): ReLU(inplace=True)
  (10): AdaptiveAvgPool2d(output_size=(1, 1))
  (11): Flatten()
  (12): Linear(in_features=64, out_features=10, bias=True)
)
Number of parameters: 724106
Epoch 0000 | Time 0.316 (0.316) | NFE-F 20.0 | NFE-B 0.0 | Train Acc 0.1209 | Test Acc 0.1254
Epoch 0001 | Time 0.340 (0.044) | NFE-F 20.2 | NFE-B 0.0 | Train Acc 0.4371 | Test Acc 0.4410
Epoch 0002 | Time 0.351 (0.038) | NFE-F 20.2 | NFE-B 0.0 | Train Acc 0.5324 | Test Acc 0.5330
Epoch 0003 | Time 0.332 (0.038) | NFE-F 20.2 | NFE-B 0.0 | Train Acc 0.5996 | Test Acc 0.5854
Epoch 0004 | Time 0.325 (0.038) | NFE-F 20.2 | NFE-B 0.0 | Train Acc 0.6502 | Test Acc 0.6353
Epoch 0005 | Time 0.364 (0.038) | NFE-F 20.2 | NFE-B 0.0 | Train Acc 0.6801 | Test Acc 0.6644
Epoch 0006 | Time 0.333 (0.038) | NFE-F 20.2 | NFE-B 0.0 | Train Acc 0.7185 | Test Acc 0.7034
Epoch 0007 | Time 0.350 (0.038) | NFE-F 20.2 | NFE-B 0.0 | Train Acc 0.7150 | Test Acc 0.6952
Epoch 0008 | Time 0.343 (0.039) | NFE-F 20.2 | NFE-B 0.0 | Train Acc 0.7662 | Test Acc 0.7388
Epoch 0009 | Time 0.542 (0.254) | NFE-F 20.2 | NFE-B 0.0 | Train Acc 0.7626 | Test Acc 0.7386
Epoch 0010 | Time 0.337 (0.063) | NFE-F 20.2 | NFE-B 0.0 | Train Acc 0.7780 | Test Acc 0.7462
Epoch 0011 | Time 0.344 (0.038) | NFE-F 20.2 | NFE-B 0.0 | Train Acc 0.7856 | Test Acc 0.7485
Epoch 0012 | Time 0.317 (0.038) | NFE-F 20.2 | NFE-B 0.0 | Train Acc 0.8168 | Test Acc 0.7842
Epoch 0013 | Time 0.332 (0.038) | NFE-F 20.2 | NFE-B 0.0 | Train Acc 0.7893 | Test Acc 0.7565
Epoch 0014 | Time 0.330 (0.038) | NFE-F 20.2 | NFE-B 0.0 | Train Acc 0.8375 | Test Acc 0.7942
Epoch 0015 | Time 0.331 (0.038) | NFE-F 20.2 | NFE-B 0.0 | Train Acc 0.8229 | Test Acc 0.7824
Epoch 0016 | Time 0.328 (0.038) | NFE-F 20.2 | NFE-B 0.0 | Train Acc 0.8463 | Test Acc 0.8020
Epoch 0017 | Time 0.355 (0.038) | NFE-F 20.2 | NFE-B 0.0 | Train Acc 0.8499 | Test Acc 0.8052
Epoch 0018 | Time 0.342 (0.038) | NFE-F 20.2 | NFE-B 0.0 | Train Acc 0.8455 | Test Acc 0.8007
Epoch 0019 | Time 0.348 (0.038) | NFE-F 20.2 | NFE-B 0.0 | Train Acc 0.8579 | Test Acc 0.8084
Epoch 0020 | Time 0.347 (0.038) | NFE-F 20.2 | NFE-B 0.0 | Train Acc 0.8388 | Test Acc 0.7838
Epoch 0021 | Time 0.320 (0.038) | NFE-F 20.2 | NFE-B 0.0 | Train Acc 0.8712 | Test Acc 0.8160
Epoch 0022 | Time 0.327 (0.038) | NFE-F 20.2 | NFE-B 0.0 | Train Acc 0.8862 | Test Acc 0.8290
Epoch 0023 | Time 0.368 (0.038) | NFE-F 20.2 | NFE-B 0.0 | Train Acc 0.8750 | Test Acc 0.8128
Epoch 0024 | Time 0.323 (0.037) | NFE-F 20.2 | NFE-B 0.0 | Train Acc 0.8918 | Test Acc 0.8271
Epoch 0025 | Time 0.337 (0.038) | NFE-F 20.2 | NFE-B 0.0 | Train Acc 0.8903 | Test Acc 0.8258
Epoch 0026 | Time 0.322 (0.038) | NFE-F 20.2 | NFE-B 0.0 | Train Acc 0.8914 | Test Acc 0.8226
Epoch 0027 | Time 0.330 (0.038) | NFE-F 20.2 | NFE-B 0.0 | Train Acc 0.8999 | Test Acc 0.8265
Epoch 0028 | Time 0.323 (0.038) | NFE-F 20.2 | NFE-B 0.0 | Train Acc 0.8961 | Test Acc 0.8240
Epoch 0029 | Time 0.320 (0.037) | NFE-F 20.2 | NFE-B 0.0 | Train Acc 0.9074 | Test Acc 0.8352
Epoch 0030 | Time 0.354 (0.038) | NFE-F 20.2 | NFE-B 0.0 | Train Acc 0.8990 | Test Acc 0.8225
Epoch 0031 | Time 0.322 (0.038) | NFE-F 20.2 | NFE-B 0.0 | Train Acc 0.9225 | Test Acc 0.8390
Epoch 0032 | Time 0.331 (0.038) | NFE-F 20.2 | NFE-B 0.0 | Train Acc 0.9225 | Test Acc 0.8358
Epoch 0033 | Time 0.354 (0.038) | NFE-F 20.2 | NFE-B 0.0 | Train Acc 0.9158 | Test Acc 0.8318
Epoch 0034 | Time 0.327 (0.038) | NFE-F 20.2 | NFE-B 0.0 | Train Acc 0.9317 | Test Acc 0.8391
Epoch 0035 | Time 0.366 (0.038) | NFE-F 20.2 | NFE-B 0.0 | Train Acc 0.9321 | Test Acc 0.8419
Epoch 0036 | Time 0.339 (0.038) | NFE-F 20.2 | NFE-B 0.0 | Train Acc 0.9363 | Test Acc 0.8416
Epoch 0037 | Time 0.315 (0.038) | NFE-F 20.2 | NFE-B 0.0 | Train Acc 0.9066 | Test Acc 0.8196
Epoch 0038 | Time 0.369 (0.038) | NFE-F 20.2 | NFE-B 0.0 | Train Acc 0.9365 | Test Acc 0.8396
Epoch 0039 | Time 0.351 (0.037) | NFE-F 20.2 | NFE-B 0.0 | Train Acc 0.9266 | Test Acc 0.8315
Epoch 0040 | Time 0.316 (0.038) | NFE-F 20.2 | NFE-B 0.0 | Train Acc 0.9341 | Test Acc 0.8376
Epoch 0041 | Time 0.317 (0.037) | NFE-F 20.2 | NFE-B 0.0 | Train Acc 0.9427 | Test Acc 0.8446
Epoch 0042 | Time 0.332 (0.038) | NFE-F 20.2 | NFE-B 0.0 | Train Acc 0.9387 | Test Acc 0.8388
Epoch 0043 | Time 0.374 (0.038) | NFE-F 20.2 | NFE-B 0.0 | Train Acc 0.9414 | Test Acc 0.8396
Epoch 0044 | Time 0.317 (0.038) | NFE-F 20.2 | NFE-B 0.0 | Train Acc 0.9430 | Test Acc 0.8381
Epoch 0045 | Time 0.304 (0.037) | NFE-F 20.2 | NFE-B 0.0 | Train Acc 0.9485 | Test Acc 0.8370
Epoch 0046 | Time 0.332 (0.038) | NFE-F 20.2 | NFE-B 0.0 | Train Acc 0.9487 | Test Acc 0.8400
Epoch 0047 | Time 0.334 (0.038) | NFE-F 20.2 | NFE-B 0.0 | Train Acc 0.9566 | Test Acc 0.8474
Epoch 0048 | Time 0.341 (0.038) | NFE-F 20.2 | NFE-B 0.0 | Train Acc 0.9442 | Test Acc 0.8338
Epoch 0049 | Time 0.338 (0.038) | NFE-F 20.2 | NFE-B 0.0 | Train Acc 0.9484 | Test Acc 0.8367
Epoch 0050 | Time 0.368 (0.037) | NFE-F 20.2 | NFE-B 0.0 | Train Acc 0.9582 | Test Acc 0.8485
Epoch 0051 | Time 0.353 (0.038) | NFE-F 20.2 | NFE-B 0.0 | Train Acc 0.9561 | Test Acc 0.8386
Epoch 0052 | Time 0.388 (0.038) | NFE-F 20.2 | NFE-B 0.0 | Train Acc 0.9522 | Test Acc 0.8401
Epoch 0053 | Time 0.329 (0.038) | NFE-F 20.2 | NFE-B 0.0 | Train Acc 0.9642 | Test Acc 0.8454
Epoch 0054 | Time 0.320 (0.038) | NFE-F 20.2 | NFE-B 0.0 | Train Acc 0.9634 | Test Acc 0.8473
Epoch 0055 | Time 0.754 (0.046) | NFE-F 20.2 | NFE-B 0.0 | Train Acc 0.9619 | Test Acc 0.8420
Epoch 0056 | Time 0.654 (0.069) | NFE-F 20.2 | NFE-B 0.0 | Train Acc 0.9682 | Test Acc 0.8468
Epoch 0057 | Time 0.352 (0.040) | NFE-F 20.2 | NFE-B 0.0 | Train Acc 0.9623 | Test Acc 0.8454
Epoch 0058 | Time 0.359 (0.038) | NFE-F 20.2 | NFE-B 0.0 | Train Acc 0.9634 | Test Acc 0.8442
Epoch 0059 | Time 0.343 (0.038) | NFE-F 20.2 | NFE-B 0.0 | Train Acc 0.9710 | Test Acc 0.8456
Epoch 0060 | Time 0.400 (0.039) | NFE-F 20.2 | NFE-B 0.0 | Train Acc 0.9645 | Test Acc 0.8402
Epoch 0061 | Time 0.344 (0.038) | NFE-F 20.2 | NFE-B 0.0 | Train Acc 0.9864 | Test Acc 0.8610
Epoch 0062 | Time 0.308 (0.038) | NFE-F 20.2 | NFE-B 0.0 | Train Acc 0.9883 | Test Acc 0.8623
Epoch 0063 | Time 0.369 (0.038) | NFE-F 20.2 | NFE-B 0.0 | Train Acc 0.9891 | Test Acc 0.8610
Epoch 0064 | Time 0.405 (0.038) | NFE-F 20.2 | NFE-B 0.0 | Train Acc 0.9901 | Test Acc 0.8623
Epoch 0065 | Time 0.339 (0.038) | NFE-F 20.2 | NFE-B 0.0 | Train Acc 0.9907 | Test Acc 0.8633
Epoch 0066 | Time 0.338 (0.038) | NFE-F 20.2 | NFE-B 0.0 | Train Acc 0.9913 | Test Acc 0.8643
Epoch 0067 | Time 0.329 (0.038) | NFE-F 20.2 | NFE-B 0.0 | Train Acc 0.9915 | Test Acc 0.8634
Epoch 0068 | Time 0.303 (0.038) | NFE-F 20.2 | NFE-B 0.0 | Train Acc 0.9923 | Test Acc 0.8633
Epoch 0069 | Time 0.350 (0.038) | NFE-F 20.2 | NFE-B 0.0 | Train Acc 0.9925 | Test Acc 0.8647
Epoch 0070 | Time 0.392 (0.039) | NFE-F 20.2 | NFE-B 0.0 | Train Acc 0.9921 | Test Acc 0.8617
Epoch 0071 | Time 0.327 (0.038) | NFE-F 20.2 | NFE-B 0.0 | Train Acc 0.9923 | Test Acc 0.8617
Epoch 0072 | Time 0.340 (0.039) | NFE-F 20.2 | NFE-B 0.0 | Train Acc 0.9929 | Test Acc 0.8640
Epoch 0073 | Time 0.357 (0.038) | NFE-F 20.2 | NFE-B 0.0 | Train Acc 0.9924 | Test Acc 0.8616
Epoch 0074 | Time 0.359 (0.038) | NFE-F 20.2 | NFE-B 0.0 | Train Acc 0.9932 | Test Acc 0.8646
Epoch 0075 | Time 0.343 (0.038) | NFE-F 20.2 | NFE-B 0.0 | Train Acc 0.9931 | Test Acc 0.8641
Epoch 0076 | Time 0.381 (0.039) | NFE-F 20.2 | NFE-B 0.0 | Train Acc 0.9934 | Test Acc 0.8630
Epoch 0077 | Time 0.338 (0.038) | NFE-F 20.2 | NFE-B 0.0 | Train Acc 0.9936 | Test Acc 0.8628
Epoch 0078 | Time 0.345 (0.038) | NFE-F 20.2 | NFE-B 0.0 | Train Acc 0.9937 | Test Acc 0.8622
Epoch 0079 | Time 0.377 (0.039) | NFE-F 20.2 | NFE-B 0.0 | Train Acc 0.9939 | Test Acc 0.8637
Epoch 0080 | Time 0.311 (0.038) | NFE-F 20.2 | NFE-B 0.0 | Train Acc 0.9942 | Test Acc 0.8633
Epoch 0081 | Time 0.375 (0.038) | NFE-F 20.2 | NFE-B 0.0 | Train Acc 0.9943 | Test Acc 0.8622
Epoch 0082 | Time 0.343 (0.038) | NFE-F 20.2 | NFE-B 0.0 | Train Acc 0.9947 | Test Acc 0.8635
Epoch 0083 | Time 0.386 (0.038) | NFE-F 20.2 | NFE-B 0.0 | Train Acc 0.9949 | Test Acc 0.8639
Epoch 0084 | Time 0.356 (0.038) | NFE-F 20.2 | NFE-B 0.0 | Train Acc 0.9953 | Test Acc 0.8629
Epoch 0085 | Time 0.339 (0.038) | NFE-F 20.2 | NFE-B 0.0 | Train Acc 0.9949 | Test Acc 0.8620
Epoch 0086 | Time 0.359 (0.038) | NFE-F 20.2 | NFE-B 0.0 | Train Acc 0.9952 | Test Acc 0.8647
Epoch 0087 | Time 0.332 (0.038) | NFE-F 20.2 | NFE-B 0.0 | Train Acc 0.9950 | Test Acc 0.8639
Epoch 0088 | Time 0.352 (0.038) | NFE-F 20.2 | NFE-B 0.0 | Train Acc 0.9952 | Test Acc 0.8635
Epoch 0089 | Time 0.339 (0.038) | NFE-F 20.2 | NFE-B 0.0 | Train Acc 0.9955 | Test Acc 0.8649
Epoch 0090 | Time 0.351 (0.038) | NFE-F 20.2 | NFE-B 0.0 | Train Acc 0.9956 | Test Acc 0.8642
Epoch 0091 | Time 0.332 (0.038) | NFE-F 20.2 | NFE-B 0.0 | Train Acc 0.9956 | Test Acc 0.8649
Epoch 0092 | Time 0.371 (0.038) | NFE-F 20.2 | NFE-B 0.0 | Train Acc 0.9957 | Test Acc 0.8648
Epoch 0093 | Time 0.568 (0.255) | NFE-F 20.2 | NFE-B 0.0 | Train Acc 0.9956 | Test Acc 0.8636
Epoch 0094 | Time 0.343 (0.073) | NFE-F 20.2 | NFE-B 0.0 | Train Acc 0.9956 | Test Acc 0.8637
Epoch 0095 | Time 0.349 (0.040) | NFE-F 20.2 | NFE-B 0.0 | Train Acc 0.9960 | Test Acc 0.8625
Epoch 0096 | Time 0.353 (0.038) | NFE-F 20.2 | NFE-B 0.0 | Train Acc 0.9962 | Test Acc 0.8645
Epoch 0097 | Time 0.365 (0.038) | NFE-F 20.2 | NFE-B 0.0 | Train Acc 0.9961 | Test Acc 0.8633
Epoch 0098 | Time 0.359 (0.037) | NFE-F 20.2 | NFE-B 0.0 | Train Acc 0.9959 | Test Acc 0.8627
Epoch 0099 | Time 0.374 (0.038) | NFE-F 20.2 | NFE-B 0.0 | Train Acc 0.9955 | Test Acc 0.8631
Epoch 0100 | Time 0.312 (0.038) | NFE-F 20.2 | NFE-B 0.0 | Train Acc 0.9964 | Test Acc 0.8645
Epoch 0101 | Time 0.330 (0.038) | NFE-F 20.2 | NFE-B 0.0 | Train Acc 0.9965 | Test Acc 0.8642
Epoch 0102 | Time 0.317 (0.038) | NFE-F 20.2 | NFE-B 0.0 | Train Acc 0.9966 | Test Acc 0.8652
Epoch 0103 | Time 0.335 (0.038) | NFE-F 20.2 | NFE-B 0.0 | Train Acc 0.9969 | Test Acc 0.8643
Epoch 0104 | Time 0.368 (0.038) | NFE-F 20.2 | NFE-B 0.0 | Train Acc 0.9967 | Test Acc 0.8645
Epoch 0105 | Time 0.350 (0.038) | NFE-F 20.2 | NFE-B 0.0 | Train Acc 0.9968 | Test Acc 0.8650
Epoch 0106 | Time 0.367 (0.037) | NFE-F 20.2 | NFE-B 0.0 | Train Acc 0.9968 | Test Acc 0.8648
Epoch 0107 | Time 0.341 (0.038) | NFE-F 20.2 | NFE-B 0.0 | Train Acc 0.9967 | Test Acc 0.8653
Epoch 0108 | Time 0.343 (0.038) | NFE-F 20.2 | NFE-B 0.0 | Train Acc 0.9969 | Test Acc 0.8639
Epoch 0109 | Time 0.377 (0.038) | NFE-F 20.2 | NFE-B 0.0 | Train Acc 0.9969 | Test Acc 0.8651
Epoch 0110 | Time 0.357 (0.038) | NFE-F 20.2 | NFE-B 0.0 | Train Acc 0.9968 | Test Acc 0.8650
Epoch 0111 | Time 0.721 (0.064) | NFE-F 20.2 | NFE-B 0.0 | Train Acc 0.9970 | Test Acc 0.8646
Epoch 0112 | Time 0.753 (0.064) | NFE-F 20.2 | NFE-B 0.0 | Train Acc 0.9970 | Test Acc 0.8664
Epoch 0113 | Time 0.848 (0.070) | NFE-F 20.2 | NFE-B 0.0 | Train Acc 0.9968 | Test Acc 0.8650
Epoch 0114 | Time 0.366 (0.039) | NFE-F 20.2 | NFE-B 0.0 | Train Acc 0.9969 | Test Acc 0.8652
Epoch 0115 | Time 0.323 (0.038) | NFE-F 20.2 | NFE-B 0.0 | Train Acc 0.9969 | Test Acc 0.8645
Epoch 0116 | Time 0.357 (0.038) | NFE-F 20.2 | NFE-B 0.0 | Train Acc 0.9970 | Test Acc 0.8650
Epoch 0117 | Time 0.346 (0.038) | NFE-F 20.2 | NFE-B 0.0 | Train Acc 0.9969 | Test Acc 0.8643
Epoch 0118 | Time 0.311 (0.038) | NFE-F 20.2 | NFE-B 0.0 | Train Acc 0.9969 | Test Acc 0.8646
Epoch 0119 | Time 0.339 (0.038) | NFE-F 20.2 | NFE-B 0.0 | Train Acc 0.9970 | Test Acc 0.8649
Epoch 0120 | Time 0.323 (0.038) | NFE-F 20.2 | NFE-B 0.0 | Train Acc 0.9970 | Test Acc 0.8642
Epoch 0121 | Time 0.381 (0.039) | NFE-F 20.2 | NFE-B 0.0 | Train Acc 0.9970 | Test Acc 0.8641
Epoch 0122 | Time 0.358 (0.038) | NFE-F 20.2 | NFE-B 0.0 | Train Acc 0.9971 | Test Acc 0.8640
Epoch 0123 | Time 0.327 (0.038) | NFE-F 20.2 | NFE-B 0.0 | Train Acc 0.9971 | Test Acc 0.8645
Epoch 0124 | Time 0.324 (0.038) | NFE-F 20.2 | NFE-B 0.0 | Train Acc 0.9970 | Test Acc 0.8638
Epoch 0125 | Time 0.347 (0.038) | NFE-F 20.2 | NFE-B 0.0 | Train Acc 0.9969 | Test Acc 0.8641
Epoch 0126 | Time 0.336 (0.038) | NFE-F 20.2 | NFE-B 0.0 | Train Acc 0.9971 | Test Acc 0.8642
Epoch 0127 | Time 0.339 (0.038) | NFE-F 20.2 | NFE-B 0.0 | Train Acc 0.9970 | Test Acc 0.8645
Epoch 0128 | Time 0.324 (0.038) | NFE-F 20.2 | NFE-B 0.0 | Train Acc 0.9971 | Test Acc 0.8638
Epoch 0129 | Time 0.384 (0.038) | NFE-F 20.2 | NFE-B 0.0 | Train Acc 0.9970 | Test Acc 0.8644
Epoch 0130 | Time 0.353 (0.038) | NFE-F 20.2 | NFE-B 0.0 | Train Acc 0.9971 | Test Acc 0.8642
Epoch 0131 | Time 0.336 (0.039) | NFE-F 20.2 | NFE-B 0.0 | Train Acc 0.9971 | Test Acc 0.8640
Epoch 0132 | Time 0.384 (0.038) | NFE-F 20.2 | NFE-B 0.0 | Train Acc 0.9973 | Test Acc 0.8644
Epoch 0133 | Time 0.325 (0.038) | NFE-F 20.2 | NFE-B 0.0 | Train Acc 0.9972 | Test Acc 0.8643
Epoch 0134 | Time 0.338 (0.038) | NFE-F 20.2 | NFE-B 0.0 | Train Acc 0.9971 | Test Acc 0.8648
Epoch 0135 | Time 0.349 (0.038) | NFE-F 20.2 | NFE-B 0.0 | Train Acc 0.9969 | Test Acc 0.8640
Epoch 0136 | Time 0.349 (0.038) | NFE-F 20.2 | NFE-B 0.0 | Train Acc 0.9969 | Test Acc 0.8646
Epoch 0137 | Time 0.396 (0.038) | NFE-F 20.2 | NFE-B 0.0 | Train Acc 0.9971 | Test Acc 0.8651
Epoch 0138 | Time 0.352 (0.038) | NFE-F 20.2 | NFE-B 0.0 | Train Acc 0.9971 | Test Acc 0.8652
Epoch 0139 | Time 0.331 (0.038) | NFE-F 20.2 | NFE-B 0.0 | Train Acc 0.9971 | Test Acc 0.8650
Epoch 0140 | Time 0.379 (0.039) | NFE-F 20.2 | NFE-B 0.0 | Train Acc 0.9971 | Test Acc 0.8646
Epoch 0141 | Time 0.378 (0.039) | NFE-F 20.2 | NFE-B 0.0 | Train Acc 0.9970 | Test Acc 0.8644
Epoch 0142 | Time 0.478 (0.039) | NFE-F 20.2 | NFE-B 0.0 | Train Acc 0.9970 | Test Acc 0.8645
Epoch 0143 | Time 0.329 (0.038) | NFE-F 20.2 | NFE-B 0.0 | Train Acc 0.9971 | Test Acc 0.8646
Epoch 0144 | Time 0.349 (0.038) | NFE-F 20.2 | NFE-B 0.0 | Train Acc 0.9970 | Test Acc 0.8650
Epoch 0145 | Time 0.339 (0.038) | NFE-F 20.2 | NFE-B 0.0 | Train Acc 0.9971 | Test Acc 0.8650
Epoch 0146 | Time 0.358 (0.038) | NFE-F 20.2 | NFE-B 0.0 | Train Acc 0.9971 | Test Acc 0.8655
Epoch 0147 | Time 0.379 (0.038) | NFE-F 20.2 | NFE-B 0.0 | Train Acc 0.9972 | Test Acc 0.8654
Epoch 0148 | Time 0.324 (0.039) | NFE-F 20.2 | NFE-B 0.0 | Train Acc 0.9971 | Test Acc 0.8653
Epoch 0149 | Time 0.384 (0.039) | NFE-F 20.2 | NFE-B 0.0 | Train Acc 0.9971 | Test Acc 0.8651
Epoch 0150 | Time 0.348 (0.038) | NFE-F 20.2 | NFE-B 0.0 | Train Acc 0.9971 | Test Acc 0.8653
Epoch 0151 | Time 0.355 (0.038) | NFE-F 20.2 | NFE-B 0.0 | Train Acc 0.9971 | Test Acc 0.8652
Epoch 0152 | Time 0.350 (0.038) | NFE-F 20.2 | NFE-B 0.0 | Train Acc 0.9971 | Test Acc 0.8653
Epoch 0153 | Time 0.400 (0.038) | NFE-F 20.2 | NFE-B 0.0 | Train Acc 0.9971 | Test Acc 0.8652
Epoch 0154 | Time 0.334 (0.038) | NFE-F 20.2 | NFE-B 0.0 | Train Acc 0.9971 | Test Acc 0.8653
Epoch 0155 | Time 0.384 (0.039) | NFE-F 20.2 | NFE-B 0.0 | Train Acc 0.9972 | Test Acc 0.8653
Epoch 0156 | Time 0.364 (0.038) | NFE-F 20.2 | NFE-B 0.0 | Train Acc 0.9972 | Test Acc 0.8653
Epoch 0157 | Time 0.322 (0.038) | NFE-F 20.2 | NFE-B 0.0 | Train Acc 0.9972 | Test Acc 0.8652
Epoch 0158 | Time 0.337 (0.038) | NFE-F 20.2 | NFE-B 0.0 | Train Acc 0.9971 | Test Acc 0.8650
Epoch 0159 | Time 0.321 (0.038) | NFE-F 20.2 | NFE-B 0.0 | Train Acc 0.9971 | Test Acc 0.8651
